{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Queston1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMyQ7V2vobiTdvdJxizfZBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvbhargava/Django-Blog-Webstite/blob/main/Queston1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary modules\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.scipy.stats import multivariate_normal\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "key = random.PRNGKey(29)"
      ],
      "metadata": {
        "id": "Z8vdTgGRkLqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is this JAX?**\n",
        "JAX is an automatic differentiation (AD) toolbox developed by a group of people at Google Brain and the open source community. It aims to bring differentiable programming in NumPy-style onto TPUs. On the highest level JAX combines the previous projects XLA & Autograd to accelorate your favorite linear algebra-based projects.\n",
        "\n",
        "Python as an interpreted programming language is slow by nature. It translates one program statement to machine code at a time and computations may get stuck in the global interpreter lock (GIL). So in order to train networks at scale we need fast compilation and parallel computing! Complied CUDA kernels for example provide a set of primitive instructions which can be executed massively parallel on a NVIDIA GPU. The computation graph generated by PyTorch or TensorFlow can then be compiled into a sequence of executions (basic operations, e.g. add/multiply) with precompiled kernels. Ideally, we want to launch as few kernels as possible because this reduces communication times and memory load. And this is where XLA comes in. It optimizes memory bandwith by “fusing” operations and reduces the amount of returned intermediate computations. In practice this can help to significantly spead up things.\n",
        "\n",
        "Autograd, on the other hand, provides automatic differentiation support for large parts of standard Python features. AD resembles the backbone of optimization in Deep Learning. It simplifies the derivative expression of a compositional function at every possible point in time. For a vast set of basic math operations we already know the functional form of their derivative. By the beauty of the chain rule, we can combine these elementary derivative and reduce the complexity of the expression at the cost of memory storage. This allows us to compute gradients which we can then use to optimize the parameters of our models using our favorite gradient-based optimization algorithm. Broadly speaking there are two types of automatic differentiation: Forward and backward mode (aka backpropagation). JAX supports AD for standard NumPy functions as well as loops which transform numerical variables.\n",
        "\n",
        "In principle these ingredients make JAX’s applicability a lot broader than Deep Learning and provide another step into the era of “Code 2.0” and differentiable programming. Many recent projects focus on DL applications (such as rlax, or haiku - two of DeepMind’s recent open source releases) but there are also other examples which benefit from buth Numba-like speed-ups with some gradient-sauce on top (e.g. Hamiltonian Monte-Carlo). "
      ],
      "metadata": {
        "id": "wLqUsBFBwNKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate Gaussian distribution\n",
        "The multivariate Gaussian distribution of an n-dimensional vector $x = (x_1 , x_2,x_3,...,x_n)$ may be written as:\n",
        "\n",
        "\\begin{align} p(\\boldsymbol{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol{\\Sigma}|}} \\exp\\left( -\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^\\mathrm{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right),\\end{align}\n",
        "\n",
        "where μ is the n-dimensional mean vector and Σ is the n×n covariance matrix.\n",
        "\n",
        "To visualize the magnitude of $p(x \\ ; \\ μ ,\\  Σ)$ as a function of all the n dimensions requires a plot in n+1 dimensions, so visualizing this distribution for n > 2 is tricky. The code below calculates and visualizes the case of n = 2, the bivariate Gaussian distribution.\n",
        "\n",
        "The plot uses the colormap viridis, which was introduced in"
      ],
      "metadata": {
        "id": "_PsPN9KTwZjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our 2-dimensional distribution will be over variables X and Y\n",
        "# Since device arrays are immutable hence we used ndarray.\n",
        "N = 60\n",
        "X = np.linspace(-3, 3, N)\n",
        "Y = np.linspace(-3, 4, N)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "\n",
        "# Mean vector and covariance matrix\n",
        "mu = jnp.array([0., 1.])\n",
        "Sigma = jnp.array([[ 1. , 0.6], [0.6,  2]])\n",
        "\n",
        "# Pack X and Y into a single 3-dimensional array\n",
        "pos = np.empty(X.shape + (2,))\n",
        "pos[:, :, 0] = X\n",
        "pos[:, :, 1] = Y\n",
        "\n",
        "def multivariate_gaussian(pos, mu, Sigma):\n",
        "    \"\"\"Return the multivariate Gaussian distribution on array pos.\n",
        "\n",
        "    pos is an array constructed by packing the meshed arrays of variables\n",
        "    x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    n = mu.shape[0]\n",
        "    Sigma_det = jnp.linalg.det(Sigma)\n",
        "    Sigma_inv = jnp.linalg.inv(Sigma)\n",
        "    N = jnp.sqrt((2*np.pi)**n * Sigma_det)\n",
        "    # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n",
        "    # way across all the input variables.\n",
        "    fac = jnp.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\n",
        "\n",
        "    return jnp.exp(-fac / 2) / N\n",
        "\n",
        "# The distribution on the variables X, Y packed into pos.\n",
        "Z = multivariate_gaussian(pos, mu, Sigma)\n",
        "\n",
        "# Create a surface plot and projected filled contour plot under it.\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "# ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
        "#                 cmap=cm.viridis)\n",
        "\n",
        "cset = ax.contourf(X, Y, Z, zdir='z', offset=-0.15, cmap=cm.viridis)\n",
        "\n",
        "\n",
        "\n",
        "# Adjust the limits, ticks and view angle\n",
        "ax.set_zlim(-0.15,0.2)\n",
        "ax.set_zticks(jnp.linspace(0,0.2,5))\n",
        "ax.view_init(27, -21)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Upy8GW_ZkjMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sigma_det = jnp.linalg.det(Sigma)\n",
        "x = jnp.linspace(-6, 7, 10, endpoint=False)\n",
        "y = multivariate_normal.pdf(x, mean=0, cov=Sigma_det); \n",
        "fig1 = plt.figure()\n",
        "ax = fig1.add_subplot(111)\n",
        "ax.plot(x, y)"
      ],
      "metadata": {
        "id": "i_gffrUipHpf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}